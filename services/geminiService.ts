import { GoogleGenAI, Part } from "@google/genai";
import { ImageFile, GenerationResult } from "../types";

// Initialize the client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

const MODEL_NAME = 'gemini-2.5-flash-image';

export const generateOrEditImage = async (
  prompt: string,
  sourceImage?: ImageFile | null
): Promise<GenerationResult> => {
  try {
    const parts: Part[] = [];

    // If there's a source image, add it to the request parts for editing
    if (sourceImage) {
      parts.push({
        inlineData: {
          data: sourceImage.data,
          mimeType: sourceImage.mimeType,
        },
      });
    }

    // Add the text prompt
    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
      model: MODEL_NAME,
      contents: { parts },
    });

    const result: GenerationResult = {};
    const candidates = response.candidates;

    if (candidates && candidates.length > 0) {
      const content = candidates[0].content;
      if (content && content.parts) {
        for (const part of content.parts) {
          if (part.inlineData) {
            const base64Data = part.inlineData.data;
            const mimeType = part.inlineData.mimeType || 'image/png';
            result.imageUrl = `data:${mimeType};base64,${base64Data}`;
          } else if (part.text) {
            // Sometimes the model might return text explanation alongside or instead of an image
            result.text = part.text;
          }
        }
      }
    }

    if (!result.imageUrl && !result.text) {
      throw new Error("No image or text generated by the model.");
    }

    return result;

  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};